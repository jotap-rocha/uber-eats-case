# Pipeline de Dados Uber Eats â€” Portfolio

Pipeline completo de engenharia de dados construido como portfolio profissional, simulando um ambiente de producao de um aplicativo de delivery (Uber Eats). O projeto implementa as melhores praticas de Data Engineering Moderna: arquitetura local automatizada (Docker + PowerShell), ingestao com Airbyte, processamento no Databricks Lakehouse com Delta Live Tables, Arquitetura Medalhao (Bronze â†’ Silver â†’ Gold), governanca com Unity Catalog e consumo em Power BI / Databricks AI/BI Genie.


---

## Arquitetura (alto nÃ­vel)
- Fontes: PostgreSQL (OLTP) e MinIO (Data Lake).
- IngestÃ£o: Airbyte para extrair e carregar os dados.
- Processamento: Databricks Lakehouse como plataforma central.
- TransformaÃ§Ã£o: Arquitetura MedalhÃ£o (Bronze â†’ Silver â†’ Gold).
- GovernanÃ§a: Unity Catalog (qualidade, linhagem e seguranÃ§a).
- Consumo: Power BI e Databricks AI/BI Genie.

## VisÃ£o Geral (infra local desta etapa)
- PostgreSQL: dados de `drivers` e `users` (tabelas criadas automaticamente por `sql/*.sql`).
- MinIO: bucket `uber-eats` para eventos JSON (console em `http://localhost:9001`).
- ShadowTraffic: geradores sintÃ©ticos para popular Postgres e MinIO.
- Scripts PowerShell automatizam setup e orquestraÃ§Ã£o (`start-all.ps1`, `stop-all.ps1`, `reset-all.ps1`).

## Requisitos de Hardware

### Minimos Recomendados
- **CPU**: Intel Core i5 8Âª geracao ou equivalente (4 cores)
- **RAM**: 16 GB (20 GB recomendado)
- **Armazenamento**: 50 GB livres (SSD preferencial)
- **Sistema Operacional**: Windows 10/11


## Pre-requisitos de Software
- Windows 10/11 com Docker Desktop instalado e em execucao
- PowerShell 5.1+ (padrao do Windows) ou PowerShell 7+ (`pwsh`)
- Acesso a internet para baixar imagens Docker
- Workspace Databricks com Unity Catalog (opcional, para proximas etapas)
- Licenca ShadowTraffic Free Trial (https://shadowtraffic.io)

## Documentacao Completa

ðŸ“š **Toda a documentacao tecnica esta em `docs/`**:

| Componente | Documentacao | Descricao |
|------------|--------------|-----------|
| **Visao Geral** | [docs/README.md](docs/README.md) | Indice completo e guia de provisionamento |
| **PostgreSQL** | [docs/postgres/README.md](docs/postgres/README.md) | Banco OLTP (drivers, users) |
| **MinIO** | [docs/minio/README.md](docs/minio/README.md) | Data Lake S3 (eventos JSON) |
| **ShadowTraffic** | [docs/shadowtraffic/README.md](docs/shadowtraffic/README.md) | Gerador de dados sinteticos |
| **Airbyte** | [docs/airbyte/README.md](docs/airbyte/README.md) | Ferramenta de ingestao |
| **Automacao** | [docs/automacao/README.md](docs/automacao/README.md) | Scripts PowerShell e Docker Compose |

## Estrutura do Projeto
```
.
â”œâ”€ docker-compose.yml
â”œâ”€ start-all.ps1
â”œâ”€ stop-all.ps1
â”œâ”€ reset-all.ps1
â”œâ”€ gen/
â”‚  â”œâ”€ .env.template        # Modelo para credenciais e variÃ¡veis
â”‚  â”œâ”€ setup-configs.ps1    # Injeta variÃ¡veis do .env nos JSONs de geradores
â”‚  â”œâ”€ minio/
â”‚  â”‚  â””â”€ uber-eats.json    # Config do gerador para MinIO
â”‚  â””â”€ postgres/
â”‚     â”œâ”€ drivers.json.template
â”‚     â””â”€ users.json.template
â””â”€ sql/
	 â”œâ”€ create_drivers_table.sql
	 â”œâ”€ create_users_table.sql
	 â””â”€ cdc configure/
			â””â”€ database-cdc-config.sql
```

## ConfiguraÃ§Ã£o (apenas uma vez)
1) Crie o arquivo `.env` a partir do template:
```
copy gen\.env.template gen\.env
```
2) Edite `gen/.env` e preencha as variÃ¡veis conforme seu ambiente/licenÃ§a do ShadowTraffic. Para um ambiente local padrÃ£o, use:
- Postgres
	- `POSTGRES_HOST=localhost`
	- `POSTGRES_PORT=5432`
	- `POSTGRES_DB=ubereats_db`
	- `POSTGRES_USERNAME=usrUberEats`
	- `POSTGRES_PASSWORD=supersecret`
- MinIO
	- `AWS_REGION=us-east-1`
	- `AWS_S3_FORCE_PATH_STYLE=true`
	- `AWS_ACCESS_KEY_ID=usrUberEats`
	- `AWS_SECRET_ACCESS_KEY=supersecret`
- LicenÃ§a ShadowTraffic: preencha os campos `LICENSE_*` conforme sua licenÃ§a.

3) (Opcional) Gerar os JSONs de configuraÃ§Ã£o para os geradores de Postgres manualmente:
```
powershell -ExecutionPolicy Bypass -File .\gen\setup-configs.ps1
```
ObservaÃ§Ã£o: o `start-all.ps1` executa esse passo automaticamente antes de subir os containers.

## Como Executar
1) Certifique-se de que o Docker Desktop estÃ¡ em execuÃ§Ã£o.
2) No terminal (cmd.exe), execute o script de inicializaÃ§Ã£o:
```
powershell -ExecutionPolicy Bypass -File .\start-all.ps1
```
O script:
- Injeta segredos/variÃ¡veis de `gen/.env` nos templates de Postgres (`drivers.json` e `users.json`).
- Sobe toda a infraestrutura via `docker-compose up -d` (em background).

Dicas:
- O ambiente sobe em background (detached). Veja logs com: `docker-compose logs -f`
- Para acompanhar um serviÃ§o especÃ­fico: `docker-compose logs -f postgres-ubereats`
- A primeira execuÃ§Ã£o pode levar ~1-2 minutos (download de imagens e inicializaÃ§Ã£o do Postgres/MinIO).

## Acessos RÃ¡pidos
- Postgres: `localhost:5432` | DB: `ubereats_db` | UsuÃ¡rio: `usrUberEats` | Senha: `supersecret`
- MinIO Console: `http://localhost:9001` | UsuÃ¡rio: `usrUberEats` | Senha: `supersecret`
- Bucket MinIO: `uber-eats` (criado automaticamente pelo serviÃ§o `minio-setup`).

As tabelas `drivers` e `users` sÃ£o criadas automaticamente a partir de `sql/create_*_table.sql` quando o volume do Postgres Ã© criado pela primeira vez.

## Visualizar dados no DBeaver (PostgreSQL)
- Cliente sugerido: DBeaver (Community Edition).
- ConexÃ£o (Driver PostgreSQL):
	- Host: `localhost`
	- Port: `5432`
	- Database: `ubereats_db`
	- User: `usrUberEats`
	- Password: `supersecret`
- Dica: apÃ³s conectar, atualize o esquema pÃºblico para visualizar as tabelas `drivers` e `users`.

## Parar e Resetar
- Parar e manter os dados (volumes preservados):
```
powershell -ExecutionPolicy Bypass -File .\stop-all.ps1
```
- Reset total (DESTRUTIVO: remove volumes/dados):
```
powershell -ExecutionPolicy Bypass -File .\reset-all.ps1
```

## SoluÃ§Ã£o de Problemas
- `.env` ausente: o `setup-configs.ps1` falharÃ¡. Crie `gen/.env` a partir de `gen/.env.template` e preencha as variÃ¡veis.
- Portas em uso: verifique se as portas `5432`, `9000`, `9001` nÃ£o estÃ£o ocupadas por outros serviÃ§os.
- Docker nÃ£o iniciado: garanta que o Docker Desktop esteja rodando antes de executar os scripts.
- ExecuÃ§Ã£o de scripts bloqueada: use `-ExecutionPolicy Bypass` como mostrado nos comandos acima.
- Verificar logs especÃ­ficos:
```
docker-compose logs postgres-ubereats
docker-compose logs minio-ubereats
docker-compose logs gen-drivers
docker-compose logs gen-users
docker-compose logs gen-minio
```

## O que estÃ¡ sendo executado (docker-compose)
- `postgres-ubereats`: Postgres 15 com `wal_level=logical`, expÃµe `5432`.
- `minio-ubereats`: MinIO com API `9000` e console `9001`.
- `minio-setup`: cria o bucket `uber-eats` automaticamente.
- `gen-drivers` e `gen-users`: geradores ShadowTraffic para Postgres (usam `gen/postgres/*.json`).
- `gen-minio`: gerador ShadowTraffic para MinIO (usa `gen/minio/uber-eats.json`).

## LicenÃ§as e Credenciais
- Nunca commite arquivos `.env` ou configs geradas (`.json`) com credenciais.
- Os templates `.json.template` usam placeholders e sÃ£o seguros para versionamento.

---

## Dados Gerados

O ambiente atual possui:
- **111.348 registros** na tabela `drivers`
- **111.155 registros** na tabela `users`
- **20+ streams de eventos JSON** no bucket MinIO `uber-eats` (orders, gps, payments, etc.)

---

## Proximos Passos

1. **Ingestao (Airbyte)**: Configurar conectores Postgres â†’ Databricks e MinIO â†’ Databricks
2. **Databricks (Bronze Layer)**: Criar pipelines DLT com Auto Loader para ingestao incremental
3. **Databricks (Silver Layer)**: Limpeza, tipagem, expectations e flattening de JSONs
4. **Databricks (Gold Layer)**: Agregacoes e metricas de negocio (analytics-ready)
5. **Consumo**: Power BI e Databricks AI/BI Genie

Consulte [docs/README.md](docs/README.md) para guias detalhados de cada etapa.

---

## Suporte e Contribuicoes

- **Documentacao Completa**: Veja a pasta `docs/` para detalhes tecnicos de cada componente
- **Issues**: Relate problemas via [GitHub Issues](https://github.com/jotap-rocha/uber-eats-case/issues)
- **Duvidas**: Consulte primeiro a secao "Troubleshooting" de cada componente

---

Pronto! Com isso voce tem uma fabrica de dados local completa para testes de pipelines, CDC e integracoes, com dados sinteticos realistas.
