Documento de Arquitetura: Pipeline Uber Eats (End-to-End)
1. Vis√£o Geral
Este documento detalha o fluxo de dados do projeto Uber Eats, desde a gera√ß√£o sint√©tica at√© a camada Gold no Databricks Lakehouse, utilizando princ√≠pios de Engenharia de Dados Moderna (ELT, Medallion Architecture, e Data Governance).

2. Fase de Gera√ß√£o (ShadowTraffic)
Motor: ShadowTraffic rodando em containers Docker.

Fluxo A (Relacional): Dados transacionais (drivers, users) s√£o injetados diretamente no banco PostgreSQL.

Fluxo B (Eventos/NoSQL): Dados de eventos (pedidos, gps, pagamentos) s√£o gerados como arquivos JSON e salvos no MinIO (S3-Compatible).

3. Camada de Ingest√£o & Landing (Raw Zone)
Ferramenta: Airbyte OSS (Self-hosted).

Estrat√©gia para Postgres:

O Airbyte l√™ o Postgres (Source) e escreve no Databricks (Destination).

O dado "aterrisa" em uma Landing Zone (External Volume no MinIO) ou diretamente em tabelas Delta no schema raw do Unity Catalog.

Estrat√©gia para MinIO:

N√£o h√° movimenta√ß√£o via Airbyte. O Databricks acessa os arquivos diretamente via External Location e External Volume.

O MinIO f√≠sico funciona como a Raw Zone l√≥gica para eventos.

4. Camada de Processamento (Databricks Lakehouse)
Governan√ßa: Unity Catalog (Namespace de 3 n√≠veis: main.uber_eats.tabela).

Framework de Transforma√ß√£o: Delta Live Tables (DLT).

Leitura de Arquivos: Uso obrigat√≥rio de Auto Loader (cloudFiles) para ingest√£o incremental e eficiente da Raw para a Bronze.

5. Arquitetura Medalh√£o (Pipeline DLT)
ü•â Camada Bronze (Raw Tables)
Objetivo: C√≥pia exata (fiel) da fonte (PostgreSQL e arquivos JSON do MinIO).

Formato: Delta Lake.

Processamento: append-only. Nenhuma limpeza √© feita aqui.

ü•à Camada Silver (Cleansed & Conformed)
Objetivo: Dados limpos, tipados e enriquecidos.

A√ß√µes:

Aplica√ß√£o de Expectations (DLT) para garantir qualidade (ex: order_id n√£o nulo).

Flattening de JSONs complexos vindos do MinIO.

Joins entre dados de sensores (GPS) e dados cadastrais (Drivers).

ü•á Camada Gold (Curated Business Tables)
Objetivo: Tabelas agregadas prontas para Analytics e BI.

Exemplos: fct_pedidos, dim_motoristas, metricas_entrega_por_regiao.

Consumo: Power BI, SQL Warehouse e AI/BI Genie.

6. Conven√ß√µes de Desenvolvimento para o Agente
Modularidade: C√≥digos PySpark devem ser encapsulados em classes ou fun√ß√µes reutiliz√°veis.

DLT Patterns: Usar decorators @dlt.table e separar a l√≥gica de neg√≥cio da l√≥gica de leitura.

Performance: Evitar UDFs (User Defined Functions) desnecess√°rias; priorizar fun√ß√µes nativas do Spark SQL.

Configura√ß√£o: Referenciar sempre o databricks.yml para deploy via Asset Bundles.